{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "pRaykAACSbuY"
      },
      "source": [
        "\n",
        "\n",
        "# Building a healthcare chatbot with Mixtral 8x7b, Haystack, and PubMed\n",
        "\n",
        "\n",
        "*notebook by Tilde Thurium:\n",
        " [Mastodon](https://tech.lgbt/@annthurium) || [Twitter](https://twitter.com/annthurium) || [LinkedIn](https://www.linkedin.com/in/annthurium/)*\n",
        "\n",
        "##Introduction\n",
        "**ðŸ“š Check out the [Building a healthcare chatbot with Mixtral 8x7b, Haystack, and PubMed](https://haystack.deepset.ai/blog/mixtral-8x7b-healthcare-chatbot) article for a detailed run through of this example.**\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "*   [HuggingFace Access Token](https://huggingface.co/settings/tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kobrp6O3SbuY"
      },
      "source": [
        "## Installing Haystack\n",
        "\n",
        "To start, let's install the latest release of Haystack with `pip`, as well as any other libraries we're going to need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFb8fAhmSbuY",
        "outputId": "2e61d75e-2b70-42a3-ee38-cc7e683c9844"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "pip install haystack-ai\n",
        "pip install pymed\n",
        "pip install huggingface_hub\n",
        "pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTiZXeHbpN1I"
      },
      "source": [
        "This asks for keys the notebook needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOynyZ__t_X5",
        "outputId": "98cf3e1f-81a1-428f-d87c-cb37a0062dd1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For the manual token authentication\n",
        "# from getpass import getpass\n",
        "# huggingface_token = getpass(\"Enter your Hugging Face api token:\")\n",
        "\n",
        "# For automatic token authentication\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rc2FY_Cvq8m"
      },
      "source": [
        "## PubMed Fetcher\n",
        "\n",
        "PubMed is the best source of up to date medical research. Now we are going to write our own custom class to pull scientific papers from PubMed that are relevant to the query at hand.\n",
        "\n",
        "The PubMed sdk basically just wraps the PubMed API so it's easier to query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FELyF-Z0NT4X"
      },
      "outputs": [],
      "source": [
        "# pymed â†’ A library for querying PubMed articles.\n",
        "# List â†’ Used for type hinting to specify lists.\n",
        "# haystack.component â†’ Defines reusable Haystack components.\n",
        "# haystack.Document â†’ A class that stores the text content and metadata of a document.\n",
        "from pymed import PubMed\n",
        "from typing import List\n",
        "from haystack import component\n",
        "from haystack import Document\n",
        "import os\n",
        "\n",
        "# Run \"export NCBI_API_KEY=\"your_actual_api_key\" with the actual api key on terminal\n",
        "# NCBI_API_KEY variable will be generated only in the current terminal session\n",
        "# Retrieve API key from environment variable\n",
        "api_key = os.getenv(\"NCBI_API_KEY\")  \n",
        "\n",
        "# Creates an instance of the PubMed API client with a user-defined identifier to track API usage.\n",
        "pubmed = PubMed(tool = \"PubMed_ChatBot\", email = \"hxb294@case.edu\")\n",
        "pubmed.api_key = api_key  # Securely set API key\n",
        "\n",
        "\n",
        "# Takes a PubMed article and converts it into a Haystack Document.\n",
        "def documentize(article):\n",
        "  return Document(content = article.abstract, meta = {'title': article.title, 'keywords': article.keywords})\n",
        "\n",
        "# max_results defines the maximum number of articles to fetch per query.\n",
        "# Modify this value to adjust how many articles are retrieved for each search term.\n",
        "max_results = 1  \n",
        "\n",
        "# Defines a custom Haystack component that fetches articles from PubMed.\n",
        "@component\n",
        "class PubMedFetcher():\n",
        "\n",
        "  # Specifies that the run() function outputs a list of Haystack Document objects.\n",
        "  @component.output_types(articles = List[Document])\n",
        "  \n",
        "  # queries: list[str] â†’ Expects a list of search queries.\n",
        "  def run(self, queries: list[str]):  \n",
        "\n",
        "    # queries[0].strip().split('\\n') â†’ Takes the first query, removes extra spaces, and splits it by new lines to handle multiple queries.\n",
        "    # queries = [\"   diabetes research \\n   insulin therapy \\n\"] -> [\"diabetes research\", \"insulin therapy\"]\n",
        "    cleaned_queries = queries[0].strip().split('\\n')\n",
        "\n",
        "    # Loops through each query(keywords) and fetches a max_results number of articles per query.\n",
        "    # The pubmed.query(query, max_results=1) sends a query to PubMed based on the keyword(s) in the query string.\n",
        "\t  # max_results=1 limits the number of results returned by PubMed to 1 (you can increase this number to fetch more results).\n",
        "\t  # The response variable contains the list of articles retrieved by PubMed based on the given query or keyword(s).\n",
        "    # Converts the articles into Haystack Document objects using documentize().\n",
        "    # Appends the processed articles to the articles list.\n",
        "    # queries = [\"cancer treatment\\nimmunotherapy\\nlung cancer\"] -> cleaned_queries = [\"cancer treatment\", \"immunotherapy\", \"lung cancer\"] -> response = pubmed.query(\"cancer treatment\", max_results=1) / response = pubmed.query(\"immunotherapy\", max_results=1) / response = pubmed.query(\"lung cancer\", max_results=1)\n",
        "    articles = []\n",
        "    \n",
        "    try:\n",
        "      for query in cleaned_queries:\n",
        "        response = pubmed.query(query, max_results)\n",
        "        '''\n",
        "        documents = []\n",
        "        for article in response:\n",
        "          documents.append(documentize(article))\n",
        "        '''\n",
        "        documents = [documentize(article) for article in response]\n",
        "        articles.extend(documents)\n",
        "    \n",
        "    # Catches any errors that occur while querying PubMed and prints an error message.\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(f\"Couldn't fetch articles for queries: {queries}\" )\n",
        "    \n",
        "    # Returns a dictionary containing a list of articles in Haystack Document format.\n",
        "    '''\n",
        "    {\n",
        "      'articles': [<list of Document objects>]  \n",
        "    }\n",
        "    '''\n",
        "    results = {'articles': articles}\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwK9hBa-k98w"
      },
      "source": [
        "Now we add our `PubmedFetcher` into a pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "lKvIKg6S1VRG",
        "outputId": "12ad199a-d72b-4c0e-b423-272119fcb422"
      },
      "outputs": [],
      "source": [
        "# HuggingFaceTGIGenerator is used to interact with Hugging Face models (e.g., Mixtral-8x7B-Instruct-v0.1).\n",
        "# Pipeline is the main structure of Haystack that allows you to chain multiple components (e.g., fetcher, keyword generator, prompt builder).\n",
        "# PromptBuilder is used to construct prompts that are used to interact with language models.\n",
        "from haystack.components.generators import HuggingFaceTGIGenerator\n",
        "# If used manual token authentication -> from haystack.utils import Secret\n",
        "\n",
        "# This is the model used specifically for keyword generation from the question you want to ask.\n",
        "# BiomedBERT or BioBERT for the future trial\n",
        "# This initializes the Mixtral-8x7B-Instruct-v0.1 model (hosted on Hugging Face) for keyword generation.\n",
        "# The warm_up() function is used to initialize and warm up the model for quicker inference (i.e., load it into memory).\n",
        "# If used manual token authentication -> keyword_llm = HuggingFaceTGIGenerator(model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\", token = Secret.from_token(huggingface_token))\n",
        "keyword_llm = HuggingFaceTGIGenerator(model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
        "keyword_llm.warm_up()\n",
        "\n",
        "# This is the model used for generating the final answer to the userâ€™s question based on the fetched articles from PubMed.\n",
        "# ChatGPT, DeepSeek, Claude for the future trial\n",
        "# This initializes the Mixtral-8x7B-Instruct-v0.1 model again, this time for generating the final answer to the question based on the fetched articles.\n",
        "# If used manual token authentication -> llm = HuggingFaceTGIGenerator(model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\", token = Secret.from_token(huggingface_token))\n",
        "llm = HuggingFaceTGIGenerator(model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
        "llm.warm_up()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWCYd6hOpoIn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# This is a prompt template used to generate keywords from a given medical question.\n",
        "# It instructs the language model to extract 3 keywords from the question that can be used to search for relevant articles on PubMed.\n",
        "# The prompt format also includes an example for the model to follow.\n",
        "\n",
        "num_keywords = 1  # Change this value to set the number of keywords dynamically\n",
        "\n",
        "keyword_prompt_template = f\"\"\"\n",
        "Your task is to convert the following question into {num_keywords} keywords that can be used to find relevant medical research papers on PubMed.\n",
        "Here is an example:\n",
        "question: \"What are the latest treatments for major depressive disorder?\"\n",
        "keywords:\n",
        "Antidepressive Agents\n",
        "Depressive Disorder, Major\n",
        "Treatment-Resistant depression\n",
        "---\n",
        "question: {{ question }}\n",
        "keywords:\n",
        "\"\"\"\n",
        "\n",
        "# This template is used to create a prompt that asks the model to answer a medical question based on a list of articles fetched from PubMed.\n",
        "# The model is instructed to use the content and keywords of the articles as context to generate an accurate answer.\n",
        "prompt_template = \"\"\"\n",
        "Answer the question truthfully based on the given documents.\n",
        "If the documents don't contain an answer, use your existing knowledge base.\n",
        "\n",
        "q: {{ question }}\n",
        "Articles:\n",
        "{% for article in articles %}\n",
        "  {{article.content}}\n",
        "  keywords: {{article.meta['keywords']}}\n",
        "  title: {{article.meta['title']}}\n",
        "{% endfor %}\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.components.builders.prompt_builder import PromptBuilder\n",
        "\n",
        "# Pipeline: This creates a sequence of steps where data flows through each component.\n",
        "# The components are connected in the following order:\n",
        "\n",
        "# 1.\tkeyword_prompt_builder generates a prompt based on the question.\n",
        "# 2.\tkeyword_llm generates keywords from the question.\n",
        "# 3.\tpubmed_fetcher uses the generated keywords to search PubMed and retrieve relevant articles.\n",
        "# 4.\tprompt_builder formats the articles as input for the final answer.\n",
        "# 5.\tllm generates the final answer based on the context provided by the articles.\n",
        "\n",
        "keyword_prompt_builder = PromptBuilder(template = keyword_prompt_template)\n",
        "prompt_builder = PromptBuilder(template = prompt_template)\n",
        "\n",
        "fetcher = PubMedFetcher()\n",
        "\n",
        "pipe = Pipeline()\n",
        "\n",
        "# This method registers different components in the pipeline. Each component performs a specific task.\n",
        "# pipe.add_component(\"keyword_prompt_builder\", keyword_prompt_builder)\n",
        "# This registers the keyword_prompt_builder under the name \"keyword_prompt_builder\", so it can be referenced later.\n",
        "pipe.add_component(\"keyword_prompt_builder\", keyword_prompt_builder)\n",
        "pipe.add_component(\"keyword_llm\", keyword_llm)\n",
        "pipe.add_component(\"pubmed_fetcher\", fetcher)\n",
        "pipe.add_component(\"prompt_builder\", prompt_builder)\n",
        "pipe.add_component(\"llm\", llm)\n",
        "\n",
        "# This method links the output of one component to the input of another. It ensures data flows through the pipeline in the correct order.\n",
        "# pipe.connect(\"keyword_prompt_builder.prompt\", \"keyword_llm.prompt\")\n",
        "# This connects the keyword_prompt_builderâ€™s prompt output to keyword_llmâ€™s prompt input.\n",
        "pipe.connect(\"keyword_prompt_builder.prompt\", \"keyword_llm.prompt\")\n",
        "pipe.connect(\"keyword_llm.replies\", \"pubmed_fetcher.queries\")\n",
        "pipe.connect(\"pubmed_fetcher.articles\", \"prompt_builder.articles\")\n",
        "pipe.connect(\"prompt_builder.prompt\", \"llm.prompt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1hTVeYpmVRH"
      },
      "source": [
        "\n",
        "While we're at it, let's make an `ask` method to wrap our query fetching. This method makes it easy to pull the query response out of the results and highlighting the answer in blue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6y2xvz2VJOT"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "# Controls how many tokens the model will generate as a response. A token in NLP (Natural Language Processing) is usually a word or part of a word.\n",
        "max_new_tokens = 500\n",
        "\n",
        "# This function takes a question, runs it through the pipeline, and generates an answer.\n",
        "# The result is printed out, showing the question followed by the generated answer.\n",
        "def ask(question):\n",
        "  output = pipe.run(data = {\"keyword_prompt_builder\": {\"question\": question},\n",
        "                            \"prompt_builder\": {\"question\": question},\n",
        "                            \"llm\": {\"generation_kwargs\": {\"max_new_tokens\": max_new_tokens}}})\n",
        "  print(question)\n",
        "  print(output['llm']['replies'][0])\n",
        "  # display(HTML(f'<div style=\"color: blue\">{output[\"llm\"]['replies'][0]}</div>'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEbStD7jmb_L"
      },
      "source": [
        "Give it a try!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUPghrs9Vm8w",
        "outputId": "1b6d70d6-4f99-4dd8-de3d-034754c62147"
      },
      "outputs": [],
      "source": [
        "# This is an example query passed to the pipeline to fetch relevant medical research and generate an answer.\n",
        "# The function processes the question and outputs the generated response based on the relevant articles from PubMed.\n",
        "\n",
        "ask(\"How are mRNA vaccines being used for cancer treatment?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFGaPu6yqHEs"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJG3DW3jnXXr"
      },
      "source": [
        "**What's next**\n",
        "\n",
        "In this tutorial, you learned to make a basic chatbot using a custom Haystack Retriever that pulls data from PubMed. In order to make this chatbot fancier, you could make improvements like:\n",
        "\n",
        "\n",
        "*   Adding additional data sources\n",
        "*   Giving the bot a name or personality\n",
        "*   Making it a [stateful agent so it remembers past queries](https://docs.haystack.deepset.ai/docs/agent#conversational-agent-memory)\n",
        "\n",
        "To see what else is possible with Haystack, you can [browse these tutorials](https://haystack.deepset.ai/tutorials) or check us out on [GitHub](https://github.com/deepset-ai/haystack). Thanks for reading!\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "bda33b16be7e844498c7c2d368d72665b4f1d165582b9547ed22a0249a29ca2e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
